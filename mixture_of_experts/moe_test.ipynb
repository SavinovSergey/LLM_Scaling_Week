{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget -nc https://raw.githubusercontent.com/SavinovSergey/LLM_Scaling_Week/main/mixture_of_experts/MOE_padding.py --no-check-certificate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T18:00:54.073023Z","iopub.execute_input":"2025-11-25T18:00:54.073324Z","iopub.status.idle":"2025-11-25T18:00:54.286067Z","shell.execute_reply.started":"2025-11-25T18:00:54.073302Z","shell.execute_reply":"2025-11-25T18:00:54.285098Z"}},"outputs":[{"name":"stdout","text":"--2025-11-25 18:00:54--  https://raw.githubusercontent.com/SavinovSergey/LLM_Scaling_Week/main/mixture_of_experts/MOE_padding.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3591 (3.5K) [text/plain]\nSaving to: ‘MOE_padding.py’\n\nMOE_padding.py      100%[===================>]   3.51K  --.-KB/s    in 0s      \n\n2025-11-25 18:00:54 (48.3 MB/s) - ‘MOE_padding.py’ saved [3591/3591]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport time\nfrom typing import Tuple, List, Dict\n\nfrom MOE_padding import moe_padding, torch_basic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T18:00:54.288623Z","iopub.execute_input":"2025-11-25T18:00:54.289316Z","iopub.status.idle":"2025-11-25T18:00:58.042918Z","shell.execute_reply.started":"2025-11-25T18:00:54.289269Z","shell.execute_reply":"2025-11-25T18:00:58.041823Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def verify_implementation():\n    \"\"\"Проверка корректности реализации\"\"\"\n    print(\"Проверка корректности реализации...\")\n    \n    # Тестовые данные\n    x = torch.tensor([\n        [-0.0236, -0.5368, -0.5663],\n        [ 0.7778, -0.8583, -0.1123],\n        [ 0.1981, -0.3514, -0.9443],\n        [-2.0655, -0.9424,  0.9870]\n    ], dtype=torch.float32)\n    \n    top_experts = torch.tensor([\n        [1, 3],\n        [2, 5], \n        [3, 5],\n        [2, 4]\n    ], dtype=torch.long)\n    \n    tokens_per_expert = torch.bincount(top_experts.flatten(), minlength=6)\n    \n    # Запуск обеих реализаций\n    result_moe_padding, padded_moe_padding = moe_padding(x, top_experts, tokens_per_expert, 2, 6)\n    result_basic, padded_basic = torch_basic(x, top_experts, tokens_per_expert, 2, 6)\n    \n    # Проверка padded_tokens_per_expert\n    print(\"padded_tokens_per_expert moe_padding:\", padded_moe_padding)\n    print(\"padded_tokens_per_expert basic:\", padded_basic)\n    \n    if not torch.equal(padded_moe_padding, padded_basic):\n        print(\"ОШИБКА: padded_tokens_per_expert не совпадают!\")\n        return False\n    \n    # Проверка основных результатов\n    non_zero_mask_moe_padding = result_moe_padding.abs().sum(dim=1) > 0\n    non_zero_mask_basic = result_basic.abs().sum(dim=1) > 0\n    \n    non_zero_moe_padding = result_moe_padding[non_zero_mask_moe_padding]\n    non_zero_basic = result_basic[non_zero_mask_basic]\n    \n    if not torch.allclose(non_zero_moe_padding, non_zero_basic, rtol=1e-5, atol=1e-6):\n        print(\"ОШИБКА: результаты не совпадают!\")\n        print(\"Moe_padding non-zero:\", non_zero_moe_padding)\n        print(\"Basic non-zero:\", non_zero_basic)\n        return False\n    \n    print(\"✓ Корректность проверена успешно!\")\n    return True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T18:00:58.043557Z","iopub.execute_input":"2025-11-25T18:00:58.043880Z","iopub.status.idle":"2025-11-25T18:00:58.055518Z","shell.execute_reply.started":"2025-11-25T18:00:58.043858Z","shell.execute_reply":"2025-11-25T18:00:58.054863Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def generate_performance_test_cases() -> List[Dict]:\n    \"\"\"Генерация тестовых случаев для измерения производительности\"\"\"\n    test_cases = []\n    \n    # Различные сценарии\n    scenarios = [\n        # (num_tokens, hidden_size, topk, num_experts, description)\n        (1000, 512, 2, 8, \"small_uniform\"),\n        (5000, 1024, 4, 16, \"medium_uniform\"), \n        (20000, 2048, 8, 32, \"large_uniform\"),\n        (10000, 1024, 4, 32, \"medium_imbalanced\"),\n    ]\n    \n    for num_tokens, hidden_size, topk, num_experts, desc in scenarios:\n        # Генерация данных\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        x = torch.randn(num_tokens, hidden_size, device=device)\n        \n        if \"imbalanced\" in desc:\n            # Создание неравномерного распределения\n            top_experts = torch.zeros(num_tokens, topk, dtype=torch.long, device=device)\n            popular_experts = torch.randint(0, num_experts // 4, (num_experts // 4,))\n            \n            for i in range(num_tokens):\n                if i % 3 == 0:  # 33% токенов выбирают популярных экспертов\n                    choices = torch.randint(0, len(popular_experts), (topk,))\n                    top_experts[i] = popular_experts[choices]\n                else:\n                    top_experts[i] = torch.randint(0, num_experts, (topk,))\n        else:\n            # Равномерное распределение\n            top_experts = torch.randint(0, num_experts, (num_tokens, topk), device=device)\n        \n        tokens_per_expert = torch.bincount(top_experts.flatten(), minlength=num_experts)\n        \n        test_cases.append({\n            'x': x,\n            'top_experts': top_experts,\n            'tokens_per_expert': tokens_per_expert,\n            'topk': topk,\n            'num_experts': num_experts,\n            'description': desc,\n            'num_tokens': num_tokens,\n            'hidden_size': hidden_size\n        })\n    \n    return test_cases\n\n\ndef benchmark_implementation(func, test_case, num_warmup=5, num_runs=20):\n    \"\"\"Бенчмарк реализации\"\"\"\n    device = test_case['x'].device\n    \n    # Прогрев\n    for _ in range(num_warmup):\n        _ = func(**{k: test_case[k] for k in ['x', 'top_experts', 'tokens_per_expert', 'topk', 'num_experts']})\n    \n    if device.type == 'cuda':\n        torch.cuda.synchronize()\n        start_memory = torch.cuda.memory_allocated()\n    \n    # Измерение времени\n    start_time = time.time()\n    for _ in range(num_runs):\n        result = func(**{k: test_case[k] for k in ['x', 'top_experts', 'tokens_per_expert', 'topk', 'num_experts']})\n    end_time = time.time()\n    \n    if device.type == 'cuda':\n        torch.cuda.synchronize()\n        end_memory = torch.cuda.memory_allocated()\n        memory_used = (end_memory - start_memory) / 1024**2  # MB\n    else:\n        memory_used = 0\n    \n    avg_time = (end_time - start_time) / num_runs\n    \n    return avg_time, memory_used, result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T18:00:58.056575Z","iopub.execute_input":"2025-11-25T18:00:58.057125Z","iopub.status.idle":"2025-11-25T18:00:58.078546Z","shell.execute_reply.started":"2025-11-25T18:00:58.057102Z","shell.execute_reply":"2025-11-25T18:00:58.077759Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def run_performance_comparison():\n    \"\"\"Запуск сравнения производительности\"\"\"\n    print(\"Запуск сравнения производительности...\")\n    print(\"=\" * 80)\n    \n    # Сначала проверяем корректность\n    if not verify_implementation():\n        print(\"Прерывание: реализация некорректна!\")\n        return\n    \n    # Генерация тестовых случаев\n    test_cases = generate_performance_test_cases()\n    \n    results = []\n    \n    for i, test_case in enumerate(test_cases):\n        print(f\"\\nТест {i+1}: {test_case['description']}\")\n        print(f\"  Токены: {test_case['num_tokens']}, Hidden: {test_case['hidden_size']}, \"\n              f\"TopK: {test_case['topk']}, Эксперты: {test_case['num_experts']}\")\n        \n        # Бенчмарк moe_padding\n        moe_time, moe_memory, moe_result = benchmark_implementation(moe_padding, test_case)\n        \n        # Бенчмарк torch_basic\n        basic_time, basic_memory, basic_result = benchmark_implementation(torch_basic, test_case)\n        \n        # Проверка корректности результатов\n        moe_padded, moe_counts = moe_result\n        basic_padded, basic_counts = basic_result\n        \n        counts_match = torch.equal(moe_counts, basic_counts)\n        \n        # Сравнение ненулевых элементов\n        moe_nonzero = moe_padded[moe_padded.abs().sum(dim=1) > 0]\n        basic_nonzero = basic_padded[basic_padded.abs().sum(dim=1) > 0]\n        data_match = torch.allclose(moe_nonzero, basic_nonzero, rtol=1e-5, atol=1e-6)\n        \n        # Расчет ускорения\n        speedup = basic_time / moe_time\n        \n        print(f\"  moe_padding: {moe_time*1000:.2f} ms, {moe_memory:.1f} MB\")\n        print(f\"  torch_basic: {basic_time*1000:.2f} ms, {basic_memory:.1f} MB\")\n        print(f\"  Ускорение: {speedup:.2f}x\")\n        print(f\"  Корректность: counts={counts_match}, data={data_match}\")\n        \n        results.append({\n            'test': test_case['description'],\n            'moe_padding_time': moe_time,\n            'basic_time': basic_time,\n            'speedup': speedup,\n            'moe_padding_memory': moe_memory,\n            'basic_memory': basic_memory,\n            'correct': counts_match and data_match\n        })\n    \n    # Вывод итогов\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ИТОГИ СРАВНЕНИЯ ПРОИЗВОДИТЕЛЬНОСТИ\")\n    print(\"=\" * 80)\n    \n    for result in results:\n        status = \"✓\" if result['correct'] else \"✗\"\n        print(f\"{result['test']:20} | {status} | Ускорение: {result['speedup']:6.2f}x | \"\n              f\"Время: {result['moe_padding_time']*1000:6.2f}ms vs {result['basic_time']*1000:6.2f}ms\")\n    \n    avg_speedup = torch.tensor([r['speedup'] for r in results if r['correct']]).mean().item()\n    print(f\"\\nСреднее ускорение: {avg_speedup:.2f}x\")\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T18:00:58.079429Z","iopub.execute_input":"2025-11-25T18:00:58.079677Z","iopub.status.idle":"2025-11-25T18:00:58.095380Z","shell.execute_reply.started":"2025-11-25T18:00:58.079653Z","shell.execute_reply":"2025-11-25T18:00:58.094666Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def memory_profiling():\n    \"\"\"Профилирование использования памяти\"\"\"\n    print(\"\\nПрофилирование использования памяти...\")\n    \n    # Большой тестовый случай\n    test_case = {\n        'num_tokens': 20000,\n        'hidden_size': 2048,\n        'topk': 8,\n        'num_experts': 32,\n        'description': 'memory_profiling'\n    }\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    x = torch.randn(test_case['num_tokens'], test_case['hidden_size'], device=device)\n    top_experts = torch.randint(0, test_case['num_experts'], \n                               (test_case['num_tokens'], test_case['topk']), device=device)\n    tokens_per_expert = torch.bincount(top_experts.flatten(), minlength=test_case['num_experts'])\n    \n    test_case.update({\n        'x': x,\n        'top_experts': top_experts,\n        'tokens_per_expert': tokens_per_expert\n    })\n    \n    if device.type == 'cuda':\n        torch.cuda.reset_peak_memory_stats()\n        \n        # moe_padding\n        result1 = moe_padding(**{k: test_case[k] for k in ['x', 'top_experts', 'tokens_per_expert', 'topk', 'num_experts']})\n        torch.cuda.synchronize()\n        moe_memory = torch.cuda.max_memory_allocated() / 1024**2\n        \n        torch.cuda.reset_peak_memory_stats()\n        \n        # torch_basic\n        result2 = torch_basic(**{k: test_case[k] for k in ['x', 'top_experts', 'tokens_per_expert', 'topk', 'num_experts']})\n        torch.cuda.synchronize()\n        basic_memory = torch.cuda.max_memory_allocated() / 1024**2\n        \n        print(f\"Пиковое использование памяти:\")\n        print(f\"  moe_padding: {moe_memory:.1f} MB\")\n        print(f\"  torch_basic: {basic_memory:.1f} MB\")\n        print(f\"  Экономия: {basic_memory - moe_memory:.1f} MB ({(basic_memory - moe_memory)/basic_memory*100:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T18:00:58.096125Z","iopub.execute_input":"2025-11-25T18:00:58.096368Z","iopub.status.idle":"2025-11-25T18:00:58.114426Z","shell.execute_reply.started":"2025-11-25T18:00:58.096344Z","shell.execute_reply":"2025-11-25T18:00:58.113739Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"results = run_performance_comparison()\n\nif torch.cuda.is_available():\n    memory_profiling()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T18:00:58.116429Z","iopub.execute_input":"2025-11-25T18:00:58.116749Z","iopub.status.idle":"2025-11-25T18:05:17.609492Z","shell.execute_reply.started":"2025-11-25T18:00:58.116732Z","shell.execute_reply":"2025-11-25T18:05:17.608780Z"}},"outputs":[{"name":"stdout","text":"Запуск сравнения производительности...\n================================================================================\nПроверка корректности реализации...\npadded_tokens_per_expert moe_padding: tensor([  0, 128, 128, 128, 128, 128], dtype=torch.int32)\npadded_tokens_per_expert basic: tensor([  0, 128, 128, 128, 128, 128], dtype=torch.int32)\n✓ Корректность проверена успешно!\n\nТест 1: small_uniform\n  Токены: 1000, Hidden: 512, TopK: 2, Эксперты: 8\n  moe_padding: 0.72 ms, 0.0 MB\n  torch_basic: 98.32 ms, 0.0 MB\n  Ускорение: 136.32x\n  Корректность: counts=True, data=True\n\nТест 2: medium_uniform\n  Токены: 5000, Hidden: 1024, TopK: 4, Эксперты: 16\n  moe_padding: 3.05 ms, 0.0 MB\n  torch_basic: 927.51 ms, 0.0 MB\n  Ускорение: 303.82x\n  Корректность: counts=True, data=True\n\nТест 3: large_uniform\n  Токены: 20000, Hidden: 2048, TopK: 8, Эксперты: 32\n  moe_padding: 28.68 ms, 0.0 MB\n  torch_basic: 7165.88 ms, 0.0 MB\n  Ускорение: 249.85x\n  Корректность: counts=True, data=True\n\nТест 4: medium_imbalanced\n  Токены: 10000, Hidden: 1024, TopK: 4, Эксперты: 32\n  moe_padding: 5.38 ms, 0.0 MB\n  torch_basic: 1791.19 ms, 0.0 MB\n  Ускорение: 333.00x\n  Корректность: counts=True, data=True\n\n================================================================================\nИТОГИ СРАВНЕНИЯ ПРОИЗВОДИТЕЛЬНОСТИ\n================================================================================\nsmall_uniform        | ✓ | Ускорение: 136.32x | Время:   0.72ms vs  98.32ms\nmedium_uniform       | ✓ | Ускорение: 303.82x | Время:   3.05ms vs 927.51ms\nlarge_uniform        | ✓ | Ускорение: 249.85x | Время:  28.68ms vs 7165.88ms\nmedium_imbalanced    | ✓ | Ускорение: 333.00x | Время:   5.38ms vs 1791.19ms\n\nСреднее ускорение: 255.75x\n\nПрофилирование использования памяти...\nПиковое использование памяти:\n  moe_padding: 2689.4 MB\n  torch_basic: 2694.0 MB\n  Экономия: 4.6 MB (0.2%)\n","output_type":"stream"}],"execution_count":7}]}